{
  "hash": "36374e1482c86c9c15a4dae0488a2960",
  "result": {
    "markdown": "---\ntitle: \"6 - Le modèle Reg-ARIMA\"\n---\n\n\n\n\n### Objectifs de cette séquence\n\nObjectifs : modélisation Reg-ARIMA, pré-ajustement de X13-ARIMA.\n\n\nAprès cette séquence, vous saurez :  \n\n- La structure et les fonctions d'un modèle Reg-ARIMA  \n\n- Reconnaître les modèles de JD+ à partir des diagnostics \n\n- Modifier les spécifications du modèle \n\n\n### Questions de positionnement\n\nQu'est-ce qu'un processus stationnaire ?\n\\vfill\n\nTendance, cycle, saisonnalité sont-ils des processus stationnaires ?\n\\vfill\n\nQue signifie \"ARIMA\" et que reflète un tel modèle ?\n\\vfill\n\nComment se comportent les erreurs de prévision d'un modèle ARIMA ?\n\\vfill\n\nQu'est-ce qu'un SARMA ?\n\\vfill\n\nSaurons nous \"deviner\" le comportement de la saisonnalité à travers un modèle ARIMA ?\n\\vfill\n\nQue sont les critères d'information et à quoi ça sert ?\n\n### X13-ARIMA\n\nDeux modules : \n\n- Reg-ARIMA : phase de pré-ajustement  \n    + Régression linéaire pour correction préalable des «\\ non-linéarités\\ »\n    + Modélisation ARIMA pour faire des prévisions  \n    + Deux étapes indépendantes en schéma, mais traitements itératifs !\n\n\n- X11 : phase de décomposition\n\n### La partie «\\ régression linéaire\\ » de X13-ARIMA\n\nObjectif : supprimer les «\\ non-linéarités\\ » par régression linéaire :  \n\n- outliers  \n- effets de calendrier  \n- autres régresseurs éventuels (ex : température)  \n$$\nY_t = \\sum \\hat{\\alpha}_i O_{it} + \\sum\\hat\\beta_j C_{jt} + X_t\n$$\nSérie *linéarisée* : $X_t = Y_t - \\sum \\hat{\\alpha}_i O_{it} - \\sum\\hat\\beta_j C_{jt}$\n\n**GROS résidu** de la régression\n\nN'est pas le résidu Reg-ARIMA, qui est un bruit blanc\n\nLa décomposition est réalisée sur la série linéarisée\n\n\n# Stationnarité et différenciation\n\n## Notion de stationnarité\n\n### Quelques définitions (1/2)\n\n*Série temporelle *: suite de variables aléatoires $(X_t)_t$ dont on observe une réalisation $(X_t(\\omega))_t$\n\nLa suite $(X_t)_t$ est appelée *processus stochastique*\n\n. . .\n\nUn processus est dit *stationnaire* lorsque la loi de $X_t$\tn'évolue pas dans le temps : distribution $\\forall s,\\,(X_t,\\dots,X_{t+s})$ indépendante du temps\n\n$\\implies$ série plus ou moins horizontale et de variance constante\n\n{{< fa arrow-circle-right >}} Notion pour faire l'inférence et construire un modèle ARIMA\n\n### Quelques définitions (2/2)\n\nStationnarité, hypothèse invérifiable {{< fa arrow-circle-right >}} en pratique processus *faiblement stationnaire* :  \n\n- les moments d'ordre 2 existent\n\n- espérance constante  \n\n- covariance entre $t$ et $t-h$ ne dépend pas du temps, mais de la distance $h$  \n    $\\implies$ variance constante\n\n. . .\n\nExemple : un bruit blanc, i.e. : \n\n- espérance nulle  \n\n- covariance entre $t$ et $t-h$ nulle, pour tout $h\\ne0$  \n\n- variance non nulle et constante\n\n## Repérer la stationnarité\n\n### Comment identifier une série non-stationnaire (en niveau) ?\n\n- Tracer le chronogramme\n\n- Etudier l'ACF : \n\n\t- Série non-stationnaire : tend lentement vers 0 et $\\hat\\rho(1)$ souvent positif et élevé\n\t\n\t- Série stationnaire : tend rapidement vers 0\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_Le_modele_Reg-ARIMA_files/figure-beamer/unnamed-chunk-1-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_Le_modele_Reg-ARIMA_files/figure-beamer/unnamed-chunk-2-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n## Stationnariser une série\n### La différenciation pour stabiliser le niveau\n\n- Si la série différenciée est un bruit blanc de moyenne nulle (marche aléatoire) :\n$$\n(I-B)y_t=y_t-y_{t-1}=\\varepsilon_t \\implies y_t=y_0+\\sum_{i=1}^t\\varepsilon_i\n$$\n{{< fa arrow-circle-right >}} Modèle naïf  \nGénéralement mouvement à la hausse ou à la baisse aléatoire,\n\n. . .\n\n- Si la série différenciée est un bruit blanc de moyenne non nulle (marche aléatoire avec dérive / *drift*) :\n$$\n(I-B)y_t=c+\\varepsilon_t \\implies y_t=y_0 +ct+\\sum_{i=1}^t\\varepsilon_i\n$$\n\n. . .\n\n- Parfois on a besoin de différencier plusieurs fois $(I-B)^2y_t=(y_t-y_{t-1}) -(y_{t-1}-y_{t-2})$ ou de faire une différenciation saisonnière $(I-B^m)y_t=y_t-y_{m}$\n\n. . .\n\n- Si saisonnalité importante, commencer par la différenciation saisonnière\n\n\n\n### Modèles Intégrés (1/3)\n\nSoit X, processus « tendance linéaire » :\n$$\nX_t=\\alpha+\\beta t + \\varepsilon_t\n$$\n\nCalculer l'espérance et la variance de la v.a. $X_t$ ?  \nX est stationnaire ?  \n\n. . .\n\nDifférence d'ordre 1 : $$(I-B)X_t = ?$$\n\nLa série obtenue est-elle stationnaire ?\n\n\nSi $X$ est un processus « tendance polynomiale d'ordre 2 », comment stationnariser la série ?\n\n\n### Modèles Intégrés (2/3)\n\nSoit X, processus « saisonnier stable  » :\n$$\nX_t=S_t+\\varepsilon_t\\quad\n\\text{avec}\n\\quad\n\\forall t,\\, S_t=S_{t+s}\n$$\n$X$ stationnaire ? \n\n. . .\n\nDifférence d'ordre 1, avec retard d'ordre $s$ :\n$$(I-B^s)X_t = ?$$\n\n\nLa série obtenue est-elle stationnaire ?\n\nSi $X_t = a+bt + S_t +\\varepsilon_t$, que donnerait cette différenciation ?\n\n\n### Modèles Intégrés (3/3)\n\nUne différenciation « simple » d'ordre $d$ supprime les tendances polynomiales d'ordre $d$ :\n$$\n(I-B)^dX_t\n$$\nUne différenciation « saisonnière » supprime aussi les tendances linéaires :\n$$(I-B^s)X_t$$\n\nUne différenciation « saisonnière » d'ordre $D$ plus grand que 1 est rare :\n$$(I-B^s)^DX_t$$\n\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_Le_modele_Reg-ARIMA_files/figure-beamer/unnamed-chunk-3-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_Le_modele_Reg-ARIMA_files/figure-beamer/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Exemple\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_Le_modele_Reg-ARIMA_files/figure-beamer/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n### Faut-il toujours différencier ?\n\nPour modéliser une série avec tendance on peut distinguer deux types de non-stationnarité :\n\n1. Modèle trend-stationnaire :\n$$\nX_t=a+bt+\\varepsilon_t\n$$\n2. Modèle avec racine unité\n$$\n(1-B)Y_t=b+\\eta\\implies Y_t=a+bt+\\underbrace{\\sum_{i=1}^t\\eta_t}_{\\text{tend. stochastique}}\n$$\n \nOn a $\\mathbb V [X_t]=\\mathbb V[\\varepsilon_t]=cst$ indépendante du temps mais $\\mathbb V [Y_t] = t\\mathbb V[\\eta_t]$\n\n### Exemple\n\n\\footnotesize\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_Le_modele_Reg-ARIMA_files/figure-beamer/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n# Modélisation ARIMA\n## Modélisation ARIMA\n### La partie «\\ modélisation ARIMA\\ »\n\nARIMA, modèle auto-projectif :\n$$\nX_t = f(X_{t-1}, X_{t-2}, X_{t-3},\\, \\dots,\n\\varepsilon_{t}, \\varepsilon_{t-1}, \\varepsilon_{t-2} \\,\\dots\n)\n$$\nTrouver $f$ ?\n\nSous hypothèse de stationnarité, il existe un «\\ modèle ARMA\\ » qui approche la série.\n\nConséquence (th de Wold) : erreurs de prévision se comportent comme le résidu du modèle (bruit blanc)\n\nOn privilégie les modèles avec faible nombre de paramètres.\n\nMéthode de Box et Jenkins pour estimer et juger de la qualité des modèles.\n\n\n# Construction du modèle ARIMA\n\n## Modèles AR et MA\n### Modèles Autorégressifs (AR) \n\n$B$ opérateur retard : $B(X_t) = X_{t-1}$, et $B^p(X_t) = X_{t-p}$  \n\n. . .\n\n\\ \n\n\n:::: {.columns}\n::: {.column width=\"30%\"}\nModèle *autorégressif* d'ordre $p$, $AR(p)$ :\n:::\n::: {.column width=\"70%\"}\n\\begin{align*}\n&X_t = \\phi_1X_{t-1}+\\phi_2 X_{t-2} + \\dots + \\phi_p X_{t-p} + \\varepsilon_t \\\\\n\\iff& (1 -\\phi_1 B-\\phi_2 B^2 - \\dots - \\phi_p B^p ) X_t = \\varepsilon_t \\\\\n\\iff& \\Phi(B)X_t = \\varepsilon_t\n\\end{align*}\n:::\n::::\n\n\\ \n\n. . .\n\n$\\varepsilon_t$ *innovation * du processus (bruit blanc indépendant du passé de $X$)\n\nUn AR modélise l'influence des $p$ réalisations passées sur la réalisation courante : effet mémoire\n\nExemples : \n\n- AR(1) : niveau d'un lac ;  \n- AR(2) : nombre de tâches solaires - Yules\n\n<!-- niveau d'un lac : Le niveau du lac Huron a été relevé chaque annéee de 1875 à 1972 -->\n\n\n### Modèles «\\ Moving Average\\ » (MA) \n\n:::: {.columns}\n::: {.column width=\"30%\"}\nModèle *moyenne mobile* d'ordre $q$, $MA(q)$ :\n:::\n::: {.column width=\"70%\"}\n\\begin{align*}\nX_t \n&= \\varepsilon_t - \\theta_1\\varepsilon_{t-1} - \\theta_2 \\varepsilon_{t-2} - \\dotsb - \\theta_q \\varepsilon_{t-q} \\\\\n\\iff X_t &= (1 -\\theta_1 B-\\theta_2 B^2 - \\dotsb - \\theta_q B^q ) \\varepsilon_t\\\\\n\\iff X_t &= \\Theta(B)\\varepsilon_t\n\\end{align*}\n:::\n::::\n\n\\ \n\n. . .\n\nProcessus MA toujours stationnaire\n\nRésulte d'une accumulation non persistante de \"q\" chocs indépendants\n\nPhénomènes qui fluctuent autour d'une moyenne : MA(1) avec une constante\n\nExemples :  \n\n- Jeu de fléchettes  \n\n\n### Modèles ARMA \n\nModèles $ARMA(p,q)$ : combine $AR(p)$ et $MA(q)$, sans ou avec constante\n$$\n\\Phi(B)X_t = \\Theta(B) \\varepsilon_t\n$$\n$$\n\\Phi(B)X_t = \\mu + \\Theta(B) \\varepsilon_t\n$$\n\nProcessus $ARMA$ résulte de l'effet \"mémoire\" et d'une accumulation non persistante de chocs aléatoires indépendants \n\n\n\n\n## Modèles SARMA et modèles intégrés\n\n### Modèles SARMA\n\nModèle $SARMA(P,Q)$ : $ARMA$ avec polynôme d'ordre $s$ (4 séries trimestrielles, 12 séries mensuelles) :\n$$\n\\Phi(B^s)X_t = \\Theta(B^s)\\varepsilon_t\n\\text{ ou }\\Phi_s(B)X_t = \\Theta_s(B)\\varepsilon_t\n$$\nIntérêt :\n\n- montrer autocorrélations d'ordre $s$  \n- simplifier l'écriture par factorisation\n\n. . .\n\n$ARMA(p,q)(P,Q)$ combine parties régulière et saisonnière : $ARMA(p,q)\\times SARMA(P,Q)$. \n\nIdentique à $ARMA (p+P*s, q+Q*s)$\n\nExemple série mensuelle : $ARMA (1,1)(1,1)$ = $ARMA (13,13)$  \n\n### Modèles Intégrés (1/3)\n\nSoit X, processus «\\ tendance linéaire\\ » :\n$$\nX_t=\\alpha+\\beta t + \\varepsilon_t\n$$\n\nCalculer l'espérance et la variance de la v.a. $X_t$ ?  \nX est stationnaire ?  \n\n. . .\n\nDifférence d'ordre 1 : $$(I-B)X_t = ?$$\n\nLa série obtenue est-elle stationnaire ?\n\n\nSi $X$ est un processus «\\ tendance polynomiale d'ordre 2\\ », comment stationnariser la série ?\n\n\n### Modèles Intégrés (2/3)\n\nSoit X, processus «\\ saisonnier stable \\ » :\n$$\nX_t=S_t+\\varepsilon_t\\quad\n\\text{avec}\n\\quad\n\\forall t,\\, S_t=S_{t+s}\n$$\n$X$ stationnaire ? \n\n. . .\n\nDifférence d'ordre 1, avec retard d'ordre $s$ :\n$$(I-B^s)X_t = ?$$\n\n\nLa série obtenue est-elle stationnaire ?\n\nSi $X$ comportait en plus une tendance linéaire, que donnerait cette différenciation ?\n\n\n### Modèles Intégrés (3/3)\n\nUne différenciation «\\ simple\\ » d'ordre $d$ supprime les tendances polynomiales d'ordre $d$ :\n$$\n(I-B)^dX_t\n$$\nUne différenciation «\\ saisonnière\\ » supprime aussi les tendances linéaires :\n$$(I-B^s)X_t$$\n\nUne différenciation «\\ saisonnière\\ » d'ordre $D$ plus grand que 1 est rare (dans JD+, $D\\leq 1$) :\n$$(I-B^s)^DX_t$$\n\n\n\n### Modèles ARIMA\n\n$ARIMA(p,d,q)$ modélise les séries non stationnaires avec tendance\n$$\n\\Phi(B)(I-B)^dX_t = \\Theta(B)\\varepsilon_t\n$$\n\n\n\n$ARIMA(p,d,q)(P,D,Q)$ modélise les séries avec tendance et saisonnalité\n$$\n\\Phi(B)\\Phi_s(B)(I-B)^d(I-B^s)^DX_t = \\Theta(B)\\Theta_s(B)\\varepsilon_t\n$$\nFactorisation des polynômes en $B$ de la partie *régulière* et de la partie *saisonnière*\n\n### Modèles ARIMA et saisonnalité (1/3)\n\nConsidérons la partie saisonnière d'un ARIMA :\n\n1 - Une série avec modèle $(p,d,q)(0,0,0)$ est-elle saisonnière ?\n\n2 - que dire de $(p,d,q)(0,0,Q)$ ?\n\n3 - $(p,d,q)(0,1,0)$ ?\n\n4 - $(p,d,q)(1,0,0)$ ?\n\n\n### Modèles ARIMA et saisonnalité (2/3)\n\nRéponses :\n\n1 - Non, aucune autocorrélation d'ordre $s$\n\n2 - Non, un MA reflète des fluctuations non persistantes, la saisonnalité persiste dans le temps\n\n3 - Oui, une saisonnalité stable\n\n4 - Ne sait pas, dépend de la valeur du coefficient $\\phi_s$\n\n- $\\phi_s$ petit en valeur absolue : pas de saisonnalité, phénomène non persistant qui se dissipe vite\n\n- $\\phi_s$ négatif : pas de saisonnalité, phénomène à répétitions bi-annuelles\n\n- $\\phi_s$ grand (proche de 1) et positif : série saisonnière, autocorrélations d'ordre $s$ qui décroît lentement\n\n### Modèles ARIMA et saisonnalité (3/3)\n\nDeux cas fréquents : \n\n- $(p,d,q)(0,1,1)$ saisonnalité stable en moyenne, avec des fluctuations ponctuelles du niveau de $\\theta_s$ (plus c'est grand, plus ça fluctue)\n\n- $(p,d,q)(1,0,1)$ saisonnalité évolutive avec dérive + fluctuations ponctuelles de niveau $\\theta_s$\n\n# Détermination du modèle ARIMA\n## Méthode de Box-Jenkins\n\n### Méthode de Box-Jenkins \n\n1.\tStationnariser le processus :  $d$, $D$\n\n2.\tIdentifier les ordres ARMA : $p$, $P$, $q$, $Q$\n    {{< fa arrow-circle-right >}} structure d'autocorrélation de la série\n\n3.\tEstimer les coefficients ARMA\n    {{< fa arrow-circle-right >}} degré de variabilité de la structure d'autocorrélation \n    \n4.\tValider le modèle\n    {{< fa arrow-circle-right >}} résidus = bruit blanc ?\n\n5.\tChoix du modèle (si plusieurs modèles valides)\n    {{< fa arrow-circle-right >}} critères d'information\n\n6.\tPrévision\n\n\n### Stationnarité et ACF\n\n![](img/seq_1_acf_pacf.png){width=90%}\n\n\n### Choix du modèle\n\nCritères d'information (à **minimiser**) pour comparer les modèles :\n\n- L'AIC (critère de Akaiké) :\n$$\nAIC(p,q) = -2\\ln (L) + 2*(p+q)\n$$  \n- L'AICC (corrigé pour les courtes périodes) : \n$$\nAICC(p,q) = -2\\ln (L) + 2(p+q)\n\\left(1- \\frac{n+p+1}{N_{obs}}\n\\right)^{-1}\n$$  \n- Le BIC (critère de Schwarz) :\n$$\nBIC(p,q) = -2\\ln (L) + (p+q)\\ln(N_{obs})\n$$  \nNe pas comparer des modèles d'ordre de différenciation différents\n\n# Principe de TRAMO-SEATS\n\n## TRAMO\n### Principe de TRAMO\n\nTRAMO = Time series Regression with ARIMA noise, Missing values and Outliers\n\n\nMêmes objectifs du pré-ajustement de X13-ARIMA (convergence des algorithmes dans JDemetra+ 3.0) :\n\n- corriger la série de points atypiques, des effets de calendrier et imputation des valeurs manquantes\n\n- prolonger la série\n\n- fournir à SEATS le modèle ARIMA à la base de la décomposition\n\n\n\n\n## SEATS\n### Principe de SEATS (1/3)\nSEATS = Signal Extraction in ARIMA Time Series\n\nSEATS utilise le modèle ARIMA de la série linéarisée TRAMO : \n$$\n\\underbrace{\\Phi(B)\\Phi_s(B)(I-B)^d(I-B^s)^D}_{\\Phi(B)}X_t = \\underbrace{\\Theta(B)\\Theta_s(B)}_{\\Theta(B)}\\varepsilon_t\n$$\nHypothèses :\n\n1. La série linéarisée peut être modélisée par un modèle ARIMA\n\n2. Les différentes composantes sont décorrélées et chaque composante peut être modélisée par un modèle ARIMA\n\n3. Les polynomes AR des composantes n'ont pas de racine commune\n\n### Principe de SEATS (2/3)\nOn factorise le polynôme AR $\\Phi(B)$:\n$$\n\\Phi(B) = \\phi_T(B) \\phi_S(B) \\phi_C(B)\n$$\n\n- $\\phi_T(B)$ racines correspondant à la tendance\n\n- $\\phi_S(B)$ racines correspondant à la saisonnalité\n\n- $\\phi_C(B)$ racines correspondant au cycle\n\n\n### Principe de SEATS (3/3)\n\n$X_t$ est exprimé sous la forme :\n$$\nX_t = \\frac{\\Theta(B)}{\\Phi(B)}\\varepsilon_t =\n\\underbrace{\\frac{\\theta_T(B)}{\\phi_T(B)}\\varepsilon_{T,t}}_{\\text{Tendance}}\n+\n\\underbrace{\\frac{\\theta_S(B)}{\\phi_S(B)}\\varepsilon_{S,t}}_{\\text{Saisonnalité}}\n+\n\\underbrace{\\frac{\\theta_C(B)}{\\phi_C(B)}\\varepsilon_{C,t}}_{\\text{Cycle}}\n+ \\underbrace{\\nu_t}_{\\substack{\\text{Irrégulier}\\\\\\text{(bruit}\\\\\\text{blanc)}}}\n$$\nUn modèle ARIMA est associé à chaque composante.\n\nInfinité de solutions : on retient celle qui minimise la variance de l'irrégulier\n\n{{< fa arrow-circle-right >}} Estimation par filtre de Wiener-Kolmogorov\n\n{{< fa arrow-circle-right >}} En France c'est X-13ARIMA qui est principalement utilisé (il n'y a pas de \"meilleure\" méthode)\n\n\n# Conclusion\n### Les essentiels\n\nLes séries économiques ne sont pas stationnaires, ni leur niveau, ni leurs fluctuations ne sont constants dans le temps\n\nIntégrer un processus permet de le stationnariser\n\nUn MA capte les fluctuations non persistantes autour d'un niveau constant - processus stationnaire\n\nUn AR met en évidence l'influence des réalisations passées sur la réalisation courante\n\nUn ARIMA reflète la structure des autocorrélations de la série, ainsi que le degré de sa variabilité dans le temps\n\nL'examen des résidus permet de valider les modèles, le choix \"optimal\" se fait grâce aux critères d'information\n\n### Exercices \n\nExercices : écrire les modèles Reg-ARIMA de vos séries à partir des éléments donnés par JDemetra+\n\n\n\n",
    "supporting": [
      "06_Le_modele_Reg-ARIMA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}